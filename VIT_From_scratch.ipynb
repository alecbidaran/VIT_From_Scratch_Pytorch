{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPvcCgu6cPPUokMtf+E/qwg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alecbidaran/VIT_From_Scratch_Pytorch/blob/main/VIT_From_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch_snippets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T5xh6AHefNgT",
        "outputId": "0afdf046-281d-4291-a02e-0832b890fcaf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_snippets\n",
            "  Downloading torch_snippets-0.517-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.8/76.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastcore in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (1.5.29)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (3.7.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (9.4.0)\n",
            "Requirement already satisfied: altair in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (4.2.2)\n",
            "Collecting dill (from torch_snippets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (7.34.0)\n",
            "Collecting loguru (from torch_snippets)\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (1.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (4.66.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (13.7.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (6.0.1)\n",
            "Requirement already satisfied: catalogue in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (2.0.10)\n",
            "Requirement already satisfied: confection in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (0.1.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (1.10.13)\n",
            "Collecting typing (from torch_snippets)\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: srsly in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (2.4.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (4.5.0)\n",
            "Requirement already satisfied: wasabi in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (1.1.2)\n",
            "Collecting jsonlines (from torch_snippets)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (0.4.0)\n",
            "Collecting xmltodict (from torch_snippets)\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Collecting fuzzywuzzy (from torch_snippets)\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (1.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (3.8.1)\n",
            "Collecting python-Levenshtein (from torch_snippets)\n",
            "  Downloading python_Levenshtein-0.23.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting pre-commit (from torch_snippets)\n",
            "  Downloading pre_commit-3.6.0-py2.py3-none-any.whl (204 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.0/204.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymupdf (from torch_snippets)\n",
            "  Downloading PyMuPDF-1.23.8-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (6.5.4)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from torch_snippets) (5.9.2)\n",
            "Collecting icecream (from torch_snippets)\n",
            "  Downloading icecream-2.1.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->torch_snippets) (1.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->torch_snippets) (1.11.4)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->torch_snippets) (0.19.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->torch_snippets) (4.8.0.76)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->torch_snippets) (2.31.6)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->torch_snippets) (2.0.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair->torch_snippets) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair->torch_snippets) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair->torch_snippets) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair->torch_snippets) (0.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torch_snippets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torch_snippets) (2023.3.post1)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastcore->torch_snippets) (23.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastcore->torch_snippets) (23.2)\n",
            "Collecting colorama>=0.3.9 (from icecream->torch_snippets)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from icecream->torch_snippets) (2.16.1)\n",
            "Collecting executing>=0.3.1 (from icecream->torch_snippets)\n",
            "  Downloading executing-2.0.1-py2.py3-none-any.whl (24 kB)\n",
            "Collecting asttokens>=2.0.1 (from icecream->torch_snippets)\n",
            "  Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->torch_snippets) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->torch_snippets)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->torch_snippets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->torch_snippets) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->torch_snippets) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->torch_snippets) (3.0.43)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->torch_snippets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->torch_snippets) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->torch_snippets) (4.9.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->torch_snippets) (23.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_snippets) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_snippets) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_snippets) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_snippets) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_snippets) (3.1.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch_snippets) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch_snippets) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch_snippets) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch_snippets) (0.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch_snippets) (5.5.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch_snippets) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch_snippets) (2.1.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch_snippets) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch_snippets) (0.9.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch_snippets) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->torch_snippets) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->torch_snippets) (2.19.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->torch_snippets) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->torch_snippets) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->torch_snippets) (2023.6.3)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit->torch_snippets)\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->torch_snippets)\n",
            "  Downloading identify-2.5.33-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nodeenv>=0.11.1 (from pre-commit->torch_snippets)\n",
            "  Downloading nodeenv-1.8.0-py2.py3-none-any.whl (22 kB)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->torch_snippets)\n",
            "  Downloading virtualenv-20.25.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.7 (from pymupdf->torch_snippets)\n",
            "  Downloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Levenshtein==0.23.0 (from python-Levenshtein->torch_snippets)\n",
            "  Downloading Levenshtein-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.1.0 (from Levenshtein==0.23.0->python-Levenshtein->torch_snippets)\n",
            "  Downloading rapidfuzz-3.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->torch_snippets) (3.0.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_snippets) (3.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->torch_snippets) (0.8.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair->torch_snippets) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair->torch_snippets) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair->torch_snippets) (0.15.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->torch_snippets) (4.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->torch_snippets) (0.1.2)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.5.0->nbconvert->torch_snippets) (6.1.12)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->torch_snippets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->torch_snippets) (0.2.12)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (3.2.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (2023.12.9)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->torch_snippets) (1.5.0)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->torch_snippets)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->torch_snippets) (3.13.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->torch_snippets) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->torch_snippets) (0.5.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert->torch_snippets) (23.2.1)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert->torch_snippets) (6.3.2)\n",
            "Building wheels for collected packages: typing\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26304 sha256=3f4d635d0cde9923873f97b12a1f58b941d8f991dd171b3d370a459bc033c019\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/d0/9e/1f26ebb66d9e1732e4098bc5a6c2d91f6c9a529838f0284890\n",
            "Successfully built typing\n",
            "Installing collected packages: fuzzywuzzy, distlib, xmltodict, virtualenv, typing, rapidfuzz, PyMuPDFb, nodeenv, loguru, jsonlines, jedi, identify, executing, dill, colorama, cfgv, asttokens, pymupdf, pre-commit, Levenshtein, icecream, python-Levenshtein, torch_snippets\n",
            "Successfully installed Levenshtein-0.23.0 PyMuPDFb-1.23.7 asttokens-2.4.1 cfgv-3.4.0 colorama-0.4.6 dill-0.3.7 distlib-0.3.8 executing-2.0.1 fuzzywuzzy-0.18.0 icecream-2.1.3 identify-2.5.33 jedi-0.19.1 jsonlines-4.0.0 loguru-0.7.2 nodeenv-1.8.0 pre-commit-3.6.0 pymupdf-1.23.8 python-Levenshtein-0.23.0 rapidfuzz-3.6.0 torch_snippets-0.517 typing-3.7.4.3 virtualenv-20.25.0 xmltodict-0.13.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUvJN3GwohyI",
        "outputId": "a40f35eb-33c1-46fc-b5b9-e04d3a8ba4ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m625.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "murlnX4DpH3G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms,datasets\n",
        "from torch_snippets import *\n",
        "import numpy as np\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "import torchvision.models as models\n",
        "from collections import OrderedDict\n",
        "from einops import rearrange"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform_t=transforms.Compose([transforms.Resize((32,32)),\n",
        "                               transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5),(0.5))])\n",
        "train_dataset=datasets.MNIST(root=\"./data\",download=True,train=True,transform=transform_t)\n",
        "valid_dataset=datasets.MNIST(root=\"./data\",download=True,train=False,transform=transform_t)"
      ],
      "metadata": {
        "id": "k7dcgaAwpsgD"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image,_=train_dataset[0]\n",
        "plt.imshow(image.T)\n",
        "image.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "y4edjbVWp1Kx",
        "outputId": "f2529bfb-6390-42f0-b384-43f40c559707"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 162
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patch=torch.nn.Conv2d(3,768,kernel_size=16,stride=16)\n",
        "patched=patch(image.unsqueeze(0))\n",
        "patched.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "gjGFFunvp8zK",
        "outputId": "e1f8a3d2-992f-4169-f147-b720ac5901a3"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-163-27b64815c1b2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpatched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [768, 3, 16, 16], expected input[1, 1, 32, 32] to have 3 channels, but got 1 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(key,query,value):\n",
        "  scores=torch.bmm(query,key.transpose(1,2))/math.sqrt(key.size(-1))\n",
        "  weights=torch.nn.functional.softmax(scores,dim=-1)\n",
        "  attn_output=torch.bmm(weights,value)\n",
        "  return attn_output\n",
        "\n",
        "class AttentionHead(torch.nn.Module):\n",
        "    def __init__(self, embed_dim, head_dim):\n",
        "        super().__init__()\n",
        "        self.q = torch.nn.Linear(embed_dim, head_dim)\n",
        "        self.k = torch.nn.Linear(embed_dim, head_dim)\n",
        "        self.v = torch.nn.Linear(embed_dim, head_dim)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        attn_outputs = scaled_dot_product_attention(\n",
        "            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n",
        "        return attn_outputs\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model,nhead):\n",
        "        super().__init__()\n",
        "        embed_dim = d_model\n",
        "        num_heads = nhead\n",
        "        head_dim = embed_dim // num_heads\n",
        "        self.heads = torch.nn.ModuleList([AttentionHead(embed_dim,head_dim) for _ in range(num_heads)])\n",
        "        self.output_linear = torch.nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
        "        x = self.output_linear(x)\n",
        "        return x\n",
        "class FeedForward(torch.nn.Module):\n",
        "    def __init__(self, d_model,hid_dim):\n",
        "        super().__init__()\n",
        "        self.linear_1 = torch.nn.Linear(d_model,hid_dim)\n",
        "        self.linear_2 = torch.nn.Linear(hid_dim,d_model)\n",
        "        self.gelu = torch.nn.GELU()\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "class TransformerEncoderLayer(torch.nn.Module):\n",
        "    def __init__(self, d_model,nhead,hid_dim):\n",
        "      super().__init__()\n",
        "      self.layer_norm1=torch.nn.LayerNorm(d_model)\n",
        "      self.layer_norm2=torch.nn.LayerNorm(d_model)\n",
        "      self.attention=MultiHeadAttention(d_model,nhead)\n",
        "      self.ff=FeedForward(d_model,hid_dim)\n",
        "    def forward(self,x):\n",
        "      l=self.layer_norm1(x)\n",
        "      x1=self.attention(l)+x\n",
        "      l2=self.layer_norm2(x1)\n",
        "      x2=self.ff(l2)+l2\n",
        "      return x2"
      ],
      "metadata": {
        "id": "ebWHG7BBY_1U"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class PositionalEncoding(torch.nn.Module):\n",
        "    def __init__(self,d_model,dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        p=torch.arange(48).unsqueeze(1)\n",
        "        wk=torch.exp(-torch.arange(0,d_model,2)*math.log(1000.0)/d_model)\n",
        "        pe=torch.zeros(48,1,d_model)\n",
        "        pe[:,0,0::2]=torch.sin(p*wk)\n",
        "        pe[:,0,1::2]=torch.cos(p*wk)\n",
        "        self.register_buffer('pe',pe)\n",
        "    def forward(self,x):\n",
        "      x=x+self.pe[:x.size(0)]\n",
        "      return self.dropout(x)"
      ],
      "metadata": {
        "id": "DXfgks99wb7j"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VIT(torch.nn.Module):\n",
        "  def __init__(self,d_model,nhead,n_layer,hid_dim,dropout=0.1):\n",
        "    super(VIT,self).__init__()\n",
        "    self.patch=torch.nn.Conv2d(1,768,kernel_size=16,stride=16)\n",
        "    encoder_layers=TransformerEncoderLayer(d_model,nhead,hid_dim)\n",
        "    self.transformer_encoder=torch.nn.ModuleList([encoder_layers for _ in range(n_layer)])\n",
        "    self.pos_emb=PositionalEncoding(d_model,dropout)\n",
        "    self.output=torch.nn.Linear(768,10)\n",
        "    self.nhead=nhead\n",
        "    self.init_weights()\n",
        "  def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.output.bias.data.zero_()\n",
        "        self.output.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.patch(x).transpose(-2,-1)\n",
        "    x=rearrange(x,'b c h w-> b (h w) c')\n",
        "    x=rearrange(x,'b h c ->h b c')\n",
        "    x=self.pos_emb(x)*math.sqrt(self.nhead)\n",
        "    x=rearrange(x,'h b c->b h c')\n",
        "    for layer in self.transformer_encoder:\n",
        "      x=layer(x)\n",
        "    return self.output(x[:,0,:])\n",
        "\n"
      ],
      "metadata": {
        "id": "sGkPuNYwowUV"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=VIT(d_model=768,nhead=4,n_layer=4,hid_dim=1024,dropout=0.1).to(device)\n",
        "#model(torch.zeros(32,3,32,32).to(device)).size()"
      ],
      "metadata": {
        "id": "QNeJvZCZwdsG"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn=torch.nn.CrossEntropyLoss()\n",
        "opti=torch.optim.SGD(model.parameters(),lr=0.001,momentum=0.9)\n"
      ],
      "metadata": {
        "id": "64k9BSmczwNU"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(inputs):\n",
        "  x,y=inputs\n",
        "  z=model(x.to(device))\n",
        "  opti.zero_grad()\n",
        "  loss=loss_fn(z,y.to(device))\n",
        "  acc=(torch.argmax(z,dim=-1)==y.to(device)).float().mean()\n",
        "  loss.backward()\n",
        "  torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "  opti.step()\n",
        "  return loss,acc\n",
        "@torch.no_grad()\n",
        "def validation_step(input):\n",
        "  model.eval()\n",
        "  z=model(input)\n",
        "  opti.zero_grad()\n",
        "  loss=loss_fn(z,input)\n",
        "  return loss\n",
        "\n",
        "\n",
        "num_epoch=10\n",
        "log=Report(num_epoch)"
      ],
      "metadata": {
        "id": "BynFJY3p5n1I"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
        "val_loader=DataLoader(valid_dataset,batch_size=32,shuffle=False)"
      ],
      "metadata": {
        "id": "OiHpmiMt5zK7"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epoch):\n",
        "    N = len(train_loader)\n",
        "    for ix, data in enumerate(train_loader):\n",
        "        loss,acc = train_step(data)\n",
        "        log.record(pos=(epoch + (ix+1)/N), trn_loss=loss,trn_acc=acc, end='\\r')\n",
        "    log.report_avgs(epoch+1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-qbnqyf51Vr",
        "outputId": "39090cbe-46d6-4811-dabb-511ac837931d"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1.000  trn_acc: 0.675  trn_loss: 0.982  (57.07s - 513.66s remaining)\n",
            "EPOCH: 2.000  trn_acc: 0.849  trn_loss: 0.487  (112.81s - 451.23s remaining)\n",
            "EPOCH: 3.000  trn_acc: 0.879  trn_loss: 0.398  (167.52s - 390.89s remaining)\n",
            "EPOCH: 4.000  trn_acc: 0.895  trn_loss: 0.349  (221.36s - 332.03s remaining)\n",
            "EPOCH: 5.000  trn_acc: 0.908  trn_loss: 0.306  (275.09s - 275.09s remaining)\n",
            "EPOCH: 6.000  trn_acc: 0.919  trn_loss: 0.269  (328.94s - 219.29s remaining)\n",
            "EPOCH: 7.000  trn_acc: 0.928  trn_loss: 0.243  (382.54s - 163.94s remaining)\n",
            "EPOCH: 8.000  trn_acc: 0.935  trn_loss: 0.214  (436.37s - 109.09s remaining)\n",
            "EPOCH: 9.000  trn_acc: 0.942  trn_loss: 0.196  (490.22s - 54.47s remaining)\n",
            "EPOCH: 10.000  trn_acc: 0.946  trn_loss: 0.177  (545.82s - 0.00s remaining)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image,label=next(iter(val_loader))\n",
        "with torch.no_grad():\n",
        "  pred=model(image.to(device))\n",
        "  acc=(pred.argmax(1)==label.to(device)).float().mean()\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "fH1wnNFA52tx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "7c3090d6-43eb-4871-b65c-f351d4016f49"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0.9688\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'cuda:0'\u001b[0m\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9688</span>, <span style=\"color: #808000; text-decoration-color: #808000\">device</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cuda:0'</span><span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lqyj0Eq2ituQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}